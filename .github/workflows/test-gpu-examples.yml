name: Build and run GPU integrated tests (examples)

on:
  workflow_dispatch:
  push:
    branches: ["*"]
  pull_request:
    branches: ["master", "main"]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.event_name }}
  cancel-in-progress: true

env:
  BPFTIME_VM_NAME: llvm

jobs:
  build-bpftime-gpu:
    runs-on: [self-hosted, Linux, X64, gpu]
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Detect CUDA root and export env
        shell: bash
        run: |
          set -euo pipefail
          # Prefer /usr/local/cuda symlink, then versioned installs.
          for cand in /usr/local/cuda /usr/local/cuda-12.9 /usr/local/cuda-12.8 /usr/local/cuda-12.7 /usr/local/cuda-12.6 /usr/local/cuda-12.5 /usr/local/cuda-12 /usr/local/cuda-11.8; do
            if [ -d "$cand" ] && [ -x "$cand/bin/nvcc" ]; then
              echo "BPFTIME_CUDA_ROOT=$cand" >> "$GITHUB_ENV"
              echo "$cand/bin" >> "$GITHUB_PATH"
              if [ -d "$cand/lib64" ]; then
                echo "LD_LIBRARY_PATH=$cand/lib64:${LD_LIBRARY_PATH:-}" >> "$GITHUB_ENV"
              fi
              echo "Using CUDA root: $cand"
              exit 0
            fi
          done
          if command -v nvcc >/dev/null 2>&1; then
            NVCC_PATH="$(command -v nvcc)"
            CUDA_FROM_NVCC="$(cd "$(dirname "$NVCC_PATH")/.." && pwd)"
            echo "BPFTIME_CUDA_ROOT=$CUDA_FROM_NVCC" >> "$GITHUB_ENV"
            echo "Using CUDA root from nvcc: $CUDA_FROM_NVCC"
            exit 0
          fi
          echo "No CUDA root with nvcc found under /usr/local and nvcc not in PATH."
          exit 1

      - name: Install build dependencies (apt)
        shell: bash
        run: |
          set -euo pipefail
          export DEBIAN_FRONTEND=noninteractive
          if ! command -v apt-get >/dev/null 2>&1; then
            echo "apt-get not found; please pre-install build dependencies on the self-hosted runner."
            exit 1
          fi
          SUDO=""
          if [ "$(id -u)" -ne 0 ]; then
            if command -v sudo >/dev/null 2>&1; then
              SUDO="sudo"
            else
              echo "Not running as root and sudo not available."
              exit 1
            fi
          fi
          $SUDO apt-get update
          $SUDO apt-get install -y --no-install-recommends \
            build-essential ca-certificates curl git pkg-config xz-utils \
            cmake ninja-build \
            libboost-all-dev libdw-dev libedit-dev libelf-dev libffi-dev libssl-dev libzstd-dev zlib1g-dev \
            libcurl4-openssl-dev

          # Ensure LLVM >= 15 for vm/llvm-jit (find_package(LLVM)).
          LLVM_CONFIG_BIN="llvm-config"
          LLVM_MAJOR=0
          if command -v "$LLVM_CONFIG_BIN" >/dev/null 2>&1; then
            LLVM_MAJOR="$("$LLVM_CONFIG_BIN" --version | cut -d. -f1 || echo 0)"
          fi
          if [ "${LLVM_MAJOR}" -lt 15 ]; then
            echo "llvm-config missing or too old (found major=${LLVM_MAJOR}); trying to install a newer LLVM/Clang toolchain..."
            set +e
            SELECTED=""
            for v in 19 18 17 16 15; do
              $SUDO apt-get install -y --no-install-recommends "llvm-$v" "llvm-$v-dev" "clang-$v" "lld-$v"
              if [ $? -eq 0 ] && command -v "llvm-config-$v" >/dev/null 2>&1; then
                SELECTED="$v"
                break
              fi
            done
            set -e
            if [ -z "$SELECTED" ]; then
              echo "Failed to install an LLVM toolchain >= 15 via apt-get."
              exit 1
            fi
            TOOLCHAIN_BIN_DIR="${RUNNER_TEMP:-/tmp}/bpftime-toolchain-bin"
            mkdir -p "$TOOLCHAIN_BIN_DIR"
            ln -sf "$(command -v "clang-$SELECTED")" "$TOOLCHAIN_BIN_DIR/clang"
            if command -v "clang++-$SELECTED" >/dev/null 2>&1; then
              ln -sf "$(command -v "clang++-$SELECTED")" "$TOOLCHAIN_BIN_DIR/clang++"
            fi
              ln -sf "$(command -v "llvm-config-$SELECTED")" "$TOOLCHAIN_BIN_DIR/llvm-config"
              export PATH="$TOOLCHAIN_BIN_DIR:$PATH"
              echo "$TOOLCHAIN_BIN_DIR" >> "$GITHUB_PATH"
          fi

          # Some environments ship llvm-config but not clang. Ensure clang is available.
          if ! command -v clang >/dev/null 2>&1; then
            echo "clang not found; attempting to install it..."
            if [ "${LLVM_MAJOR}" -ge 15 ] && $SUDO apt-get install -y --no-install-recommends "clang-${LLVM_MAJOR}" "lld-${LLVM_MAJOR}"; then
              TOOLCHAIN_BIN_DIR="${RUNNER_TEMP:-/tmp}/bpftime-toolchain-bin"
              mkdir -p "$TOOLCHAIN_BIN_DIR"
              ln -sf "$(command -v "clang-${LLVM_MAJOR}")" "$TOOLCHAIN_BIN_DIR/clang"
              if command -v "clang++-${LLVM_MAJOR}" >/dev/null 2>&1; then
                ln -sf "$(command -v "clang++-${LLVM_MAJOR}")" "$TOOLCHAIN_BIN_DIR/clang++"
              fi
              export PATH="$TOOLCHAIN_BIN_DIR:$PATH"
              echo "$TOOLCHAIN_BIN_DIR" >> "$GITHUB_PATH"
            else
              $SUDO apt-get install -y --no-install-recommends clang lld
            fi
          fi

          echo "LLVM_DIR=$(llvm-config --cmakedir)" >> "$GITHUB_ENV"

      - name: Check required tools
        shell: bash
        run: |
          set -euo pipefail
          for cmd in cmake ninja clang llvm-config nvcc; do
            if ! command -v "$cmd" >/dev/null 2>&1; then
              echo "Missing required tool: $cmd"
              exit 1
            fi
          done
          echo "LLVM_DIR=${LLVM_DIR:-unset}"
          llvm-config --version
          nvcc --version
          nvidia-smi -L

      - name: Ensure large /dev/shm
        shell: bash
        run: |
          set -euo pipefail
          df -h /dev/shm || true
          if mount | grep -qE " on /dev/shm( |$)"; then
            if [ "$(id -u)" -eq 0 ]; then
              mount -o remount,size=2G /dev/shm || mount -o remount,size=1G /dev/shm || true
            elif command -v sudo >/dev/null 2>&1; then
              sudo mount -o remount,size=2G /dev/shm || sudo mount -o remount,size=1G /dev/shm || true
            fi
          fi
          df -h /dev/shm || true

      - name: Build bpftime with CUDA support (LLVM JIT)
        shell: bash
        run: |
          set -euo pipefail
          cmake -B build -S . \
            -DCMAKE_BUILD_TYPE=Debug \
            -DBPFTIME_LLVM_JIT=YES \
            -DBPFTIME_ENABLE_CUDA_ATTACH=ON \
            -DBPFTIME_CUDA_ROOT="${BPFTIME_CUDA_ROOT:-/usr/local/cuda}" \
            -DLLVM_DIR="${LLVM_DIR}" \
            -G Ninja
          cmake --build build --config Debug --target bpftime-agent bpftime-syscall-server bpftimetool bpftime_text_segment_transformer nv_attach_impl_ptx_compiler ptxpass_kprobe_entry ptxpass_kretprobe ptxpass_kprobe_memcapture -j"$(nproc)"

      - name: Upload bpftime build artifact
        uses: actions/upload-artifact@v4
        with:
          name: bpftime-gpu-build
          if-no-files-found: error
          retention-days: 7
          path: |
            build/runtime/agent/libbpftime-agent.so
            build/runtime/syscall-server/libbpftime-syscall-server.so
            build/tools/bpftimetool/bpftimetool
            build/attach/text_segment_transformer
            build/attach/nv_attach_impl

  test-gpu-examples:
    needs: [build-bpftime-gpu]
    runs-on: [self-hosted, Linux, X64, gpu]
    strategy:
      fail-fast: false
      max-parallel: 1
      matrix:
        examples:
          - path: atomizer
            executable: ./atomizer
            victim: ./vec_add
            expected_str: "Exited _Z9vectorAdd"
            name: atomizer
            server_timeout: 15
            mode: two-process
          - path: kernel_trace
            executable: ./kernel_trace
            victim: ./vec_add
            expected_str: "total events:"
            name: kernel_trace
            server_timeout: 15
            mode: two-process
          - path: gpu_shared_map
            executable: ./gpu_shared_map
            victim: ./vec_add
            expected_str: "counter[0]="
            name: gpu_shared_map
            server_timeout: 15
            mode: two-process
          - path: threadscheduling
            executable: ./threadscheduling
            victim: ./vec_add
            expected_str: "SM Utilization Histogram"
            name: threadscheduling
            server_timeout: 15
            mode: two-process
          - path: cudagraph
            executable: ./cuda_probe
            victim: ./vec_add_graph
            expected_str: "calls:"
            name: cudagraph
            server_timeout: 15
            mode: two-process
          - path: threadhist-gpu-kernel-shared-map
            executable: ./threadhist
            victim: ./vec_add
            expected_str: "value(0)="
            name: threadhist-gpu-kernel-shared-map
            server_timeout: 15
            mode: two-process
          - path: directly_run_on_gpu
            executable: ./directly_run
            victim: ""
            expected_str: "GEMM C[0]="
            name: directly_run_on_gpu
            server_timeout: 15
            mode: direct-run
          - path: kernelretsnoop
            executable: ./kernelretsnoop
            victim: ./vec_add
            expected_str: "Thread ("
            name: kernelretsnoop
            server_timeout: 15
            mode: two-process
          - path: threadhist
            executable: ./threadhist
            victim: ./vec_add
            expected_str: "Thread "
            name: threadhist
            server_timeout: 15
            mode: two-process
          - path: cuda-counter
            executable: ./cuda_probe
            victim: ./vec_add
            expected_str: "calls:"
            name: cuda-counter
            server_timeout: 15
            mode: two-process
          - path: launchlate
            executable: ./launchlate
            victim: ./vec_add
            expected_str: "Monitoring CUDA kernel launch latency"
            name: launchlate
            server_timeout: 15
            mode: two-process
          - path: launchlate-kernel-gpu-shared-map
            executable: ./launchlate
            victim: ./vec_add
            expected_str: "Monitoring CUDA kernel launch latency"
            name: launchlate-kernel-gpu-shared-map
            server_timeout: 15
            mode: two-process
          - path: mem_trace
            executable: ./mem_trace
            victim: ./vec_add
            expected_str: "counter[0]="
            name: mem_trace
            server_timeout: 15
            mode: two-process
          - path: host_map_test
            executable: ./host_map_test
            victim: ./vec_add
            expected_str: "(call_count)"
            name: host_map_test
            server_timeout: 15
            mode: two-process
          - path: cutlass
            executable: ./counter/cutlass_launch_counter
            victim: ./gemm/cutlass_gemm --shape 1024x1024x1024 --launches 4 --warmup 1 --verify
            expected_str: "CUTLASS launches:"
            name: cutlass
            server_timeout: 15
            mode: two-process
            dispatch_only: true

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Download bpftime build artifact
        uses: actions/download-artifact@v4
        with:
          name: bpftime-gpu-build
          path: .
          merge-multiple: true

      - name: Normalize bpftime artifact layout
        shell: bash
        run: |
          set -euo pipefail
          # Depending on download-artifact version/config, contents may be placed under a
          # top-level directory named after the artifact (e.g. ./bpftime-gpu-build/).
          if [ -d build ]; then
            echo "Found build/ in workspace."
            exit 0
          fi

          if [ -d bpftime-gpu-build/build ]; then
            mv bpftime-gpu-build/build build
          else
            # Fallback: search for a known file and infer BUILD_DIR.
            AGENT_PATH="$(find . -maxdepth 8 -type f -path "*/runtime/agent/libbpftime-agent.so" -print -quit || true)"
            if [ -n "$AGENT_PATH" ]; then
              BUILD_DIR="$(cd "$(dirname "$AGENT_PATH")/../.." && pwd)"
              ln -s "$BUILD_DIR" build
              echo "Symlinked build -> $BUILD_DIR"
            fi
          fi

          if [ ! -e build ]; then
            echo "Downloaded artifact but could not locate a build/ directory."
            echo "Top-level entries:"; ls -la
            if [ -d bpftime-gpu-build ]; then
              echo "bpftime-gpu-build entries:"; ls -la bpftime-gpu-build || true
              echo "bpftime-gpu-build tree (depth=4):"; find bpftime-gpu-build -maxdepth 4 -mindepth 1 -print || true
            fi
            echo "Search for key files (depth=8):"
            find . -maxdepth 8 -type f \( \
              -name 'libbpftime-agent.so' -o \
              -name 'libbpftime-syscall-server.so' -o \
              -name 'bpftimetool' \
            \) -print || true
            exit 1
          fi

      - name: Detect CUDA root and export env
        shell: bash
        run: |
          set -euo pipefail
          # Prefer /usr/local/cuda symlink, then versioned installs.
          for cand in /usr/local/cuda /usr/local/cuda-12.9 /usr/local/cuda-12.8 /usr/local/cuda-12.7 /usr/local/cuda-12.6 /usr/local/cuda-12.5 /usr/local/cuda-12 /usr/local/cuda-11.8; do
            if [ -d "$cand" ] && [ -x "$cand/bin/nvcc" ]; then
              echo "BPFTIME_CUDA_ROOT=$cand" >> "$GITHUB_ENV"
              echo "$cand/bin" >> "$GITHUB_PATH"
              if [ -d "$cand/lib64" ]; then
                echo "LD_LIBRARY_PATH=$cand/lib64:${LD_LIBRARY_PATH:-}" >> "$GITHUB_ENV"
              fi
              echo "Using CUDA root: $cand"
              exit 0
            fi
          done
          if command -v nvcc >/dev/null 2>&1; then
            NVCC_PATH="$(command -v nvcc)"
            CUDA_FROM_NVCC="$(cd "$(dirname "$NVCC_PATH")/.." && pwd)"
            echo "BPFTIME_CUDA_ROOT=$CUDA_FROM_NVCC" >> "$GITHUB_ENV"
            echo "Using CUDA root from nvcc: $CUDA_FROM_NVCC"
            exit 0
          fi
          echo "No CUDA root with nvcc found under /usr/local and nvcc not in PATH."
          exit 1

      - name: Install test dependencies (apt)
        shell: bash
        run: |
          set -euo pipefail
          export DEBIAN_FRONTEND=noninteractive
          if ! command -v apt-get >/dev/null 2>&1; then
            echo "apt-get not found; please pre-install test dependencies on the self-hosted runner."
            exit 1
          fi
          SUDO=""
          if [ "$(id -u)" -ne 0 ]; then
            if command -v sudo >/dev/null 2>&1; then
              SUDO="sudo"
            else
              echo "Not running as root and sudo not available."
              exit 1
            fi
          fi
          $SUDO apt-get update
          $SUDO apt-get install -y --no-install-recommends \
            build-essential ca-certificates curl git pkg-config xz-utils \
            cmake ninja-build \
            libboost-all-dev libdw-dev libedit-dev libelf-dev libffi-dev libssl-dev libzstd-dev zlib1g-dev \
            libcurl4-openssl-dev

          # Examples build libbpf/bpftool and compile eBPF with clang.
          LLVM_CONFIG_BIN="llvm-config"
          LLVM_MAJOR=0
          if command -v "$LLVM_CONFIG_BIN" >/dev/null 2>&1; then
            LLVM_MAJOR="$("$LLVM_CONFIG_BIN" --version | cut -d. -f1 || echo 0)"
          fi
          if [ "${LLVM_MAJOR}" -lt 15 ]; then
            echo "llvm-config missing or too old (found major=${LLVM_MAJOR}); trying to install a newer LLVM/Clang toolchain..."
            set +e
            SELECTED=""
            for v in 19 18 17 16 15; do
              $SUDO apt-get install -y --no-install-recommends "llvm-$v" "llvm-$v-dev" "clang-$v" "lld-$v"
              if [ $? -eq 0 ] && command -v "llvm-config-$v" >/dev/null 2>&1; then
                SELECTED="$v"
                break
              fi
            done
            set -e
            if [ -z "$SELECTED" ]; then
              echo "Failed to install an LLVM toolchain >= 15 via apt-get."
              exit 1
            fi
            TOOLCHAIN_BIN_DIR="${RUNNER_TEMP:-/tmp}/bpftime-toolchain-bin"
            mkdir -p "$TOOLCHAIN_BIN_DIR"
            ln -sf "$(command -v "clang-$SELECTED")" "$TOOLCHAIN_BIN_DIR/clang"
            if command -v "clang++-$SELECTED" >/dev/null 2>&1; then
              ln -sf "$(command -v "clang++-$SELECTED")" "$TOOLCHAIN_BIN_DIR/clang++"
            fi
              ln -sf "$(command -v "llvm-config-$SELECTED")" "$TOOLCHAIN_BIN_DIR/llvm-config"
              export PATH="$TOOLCHAIN_BIN_DIR:$PATH"
              echo "$TOOLCHAIN_BIN_DIR" >> "$GITHUB_PATH"
          fi

          # Some environments ship llvm-config but not clang. Ensure clang is available.
          if ! command -v clang >/dev/null 2>&1; then
            echo "clang not found; attempting to install it..."
            if [ "${LLVM_MAJOR}" -ge 15 ] && $SUDO apt-get install -y --no-install-recommends "clang-${LLVM_MAJOR}" "lld-${LLVM_MAJOR}"; then
              TOOLCHAIN_BIN_DIR="${RUNNER_TEMP:-/tmp}/bpftime-toolchain-bin"
              mkdir -p "$TOOLCHAIN_BIN_DIR"
              ln -sf "$(command -v "clang-${LLVM_MAJOR}")" "$TOOLCHAIN_BIN_DIR/clang"
              if command -v "clang++-${LLVM_MAJOR}" >/dev/null 2>&1; then
                ln -sf "$(command -v "clang++-${LLVM_MAJOR}")" "$TOOLCHAIN_BIN_DIR/clang++"
              fi
              export PATH="$TOOLCHAIN_BIN_DIR:$PATH"
              echo "$TOOLCHAIN_BIN_DIR" >> "$GITHUB_PATH"
            else
              $SUDO apt-get install -y --no-install-recommends clang lld
            fi
          fi

          echo "LLVM_DIR=$(llvm-config --cmakedir)" >> "$GITHUB_ENV"

      - name: Check required tools
        shell: bash
        run: |
          set -euo pipefail
          for cmd in cmake ninja clang llvm-config nvcc; do
            if ! command -v "$cmd" >/dev/null 2>&1; then
              echo "Missing required tool: $cmd"
              exit 1
            fi
          done
          echo "LLVM_DIR=${LLVM_DIR:-unset}"
          llvm-config --version
          nvcc --version
          nvidia-smi -L

      - name: Ensure large /dev/shm
        shell: bash
        run: |
          set -euo pipefail
          df -h /dev/shm || true
          if mount | grep -qE " on /dev/shm( |$)"; then
            if [ "$(id -u)" -eq 0 ]; then
              mount -o remount,size=2G /dev/shm || mount -o remount,size=1G /dev/shm || true
            elif command -v sudo >/dev/null 2>&1; then
              sudo mount -o remount,size=2G /dev/shm || sudo mount -o remount,size=1G /dev/shm || true
            fi
          fi
          df -h /dev/shm || true

      - name: Verify bpftime build artifact
        shell: bash
        run: |
          set -euo pipefail
          if [ ! -e build ]; then
            echo "build/ does not exist after artifact download."
            ls -la
            exit 1
          fi
          missing=0
          for p in \
            build/runtime/agent/libbpftime-agent.so \
            build/runtime/syscall-server/libbpftime-syscall-server.so \
            build/tools/bpftimetool/bpftimetool \
            build/attach/text_segment_transformer \
            build/attach/nv_attach_impl; do
            if [ ! -e "$p" ]; then
              echo "Missing from downloaded artifact: $p"
              missing=1
            fi
          done
          if [ "$missing" -ne 0 ]; then
            echo "--- build/ (depth=4) ---"
            find build -maxdepth 4 -mindepth 1 -print || true
            exit 1
          fi
          test -x build/tools/bpftimetool/bpftimetool

      - name: Build GPU example
        run: |
          echo "Building GPU example: ${{matrix.examples.path}}"
          # Prefer system clang; fallback to clang-18 if needed
          if ! command -v clang >/dev/null 2>&1 && command -v clang-18 >/dev/null 2>&1; then
            export CLANG=clang-18
          fi
          # Some examples (e.g. CUTLASS) use CUDA_HOME/NVCC directly instead of PATH lookups.
          export CUDA_HOME="${BPFTIME_CUDA_ROOT:-/usr/local/cuda}"
          export NVCC="${CUDA_HOME}/bin/nvcc"
          make -C example/gpu/${{matrix.examples.path}} -j

      - name: Test GPU example (per README two-process)
        shell: bash
        run: |
          set -euo pipefail
          ROOT_DIR=$(pwd)
          EXAMPLE_DIR="$ROOT_DIR/example/gpu/${{matrix.examples.path}}"
          BUILD_DIR="$ROOT_DIR/build"
          SERVER_SO="$BUILD_DIR/runtime/syscall-server/libbpftime-syscall-server.so"
          AGENT_SO="$BUILD_DIR/runtime/agent/libbpftime-agent.so"
          EXE_REL="${{ matrix.examples.executable }}"
          VICTIM_REL="${{ matrix.examples.victim }}"
          EXPECTED="${{ matrix.examples.expected_str }}"
          EXAMPLE_NAME="${{ matrix.examples.name }}"
          EXAMPLE_MODE="${{ matrix.examples.mode }}"
          CLIENT_TIMEOUT=30
          BPFTIMETOOL="$BUILD_DIR/tools/bpftimetool/bpftimetool"
          export SPDLOG_LEVEL=info
          export BPFTIME_LOG_LEVEL=INFO
          export CUDA_LAUNCH_BLOCKING=0
          export CUDA_MODULE_LOADING=LAZY
          export BPFTIME_GLOBAL_SHM_NAME="bpftime_maps_shm_${EXAMPLE_NAME}"
          # GPU per-thread map sizing (some examples are memory-sensitive).
          GPU_THREAD_COUNT=8192
          case "$EXAMPLE_NAME" in
            kernelretsnoop) GPU_THREAD_COUNT=10 ;;       # per README: avoid bad_alloc
            threadhist) GPU_THREAD_COUNT=524288 ;;       # per README: allow large thread id space
            kernel_trace) GPU_THREAD_COUNT=4096 ;;       # kernel_trace vec_add launches 32*128=4096 threads
          esac
          export BPFTIME_MAP_GPU_THREAD_COUNT="$GPU_THREAD_COUNT"
          export LD_LIBRARY_PATH="$ROOT_DIR/build/attach/text_segment_transformer:${LD_LIBRARY_PATH:-}"
          df -h /dev/shm || true
          # Best-effort cleanup in case a previous job was interrupted.
          # Note: `pkill -f` matches argv only (not environment variables),
          # so we must kill known binaries explicitly.
          for proc in atomizer kernel_trace gpu_shared_map threadscheduling cuda_probe threadhist directly_run kernelretsnoop launchlate mem_trace vec_add vec_add_graph cutlass_launch_counter cutlass_gemm; do
            pkill -9 -x "$proc" 2>/dev/null || true
          done
          SERVER_BIN="$(basename "${EXE_REL%% *}")"
          pkill -9 -x "$SERVER_BIN" 2>/dev/null || true
          if [ -n "${VICTIM_REL:-}" ]; then
            VICTIM_BIN="$(basename "${VICTIM_REL%% *}")"
            pkill -9 -x "$VICTIM_BIN" 2>/dev/null || true
          fi
          # Remove bpftime shared memory segments and semaphores
          rm -f /dev/shm/bpftime* || true
          rm -f /dev/shm/sem.bpftime* || true
          # Re-check after cleanup: lingering processes can keep shm pinned.
          df -h /dev/shm || true
          # ensure previous logs do not interfere
          rm -f server.log client.log tool.log || true
          : > server.log
          : > client.log
          : > tool.log
          # tune shared memory size per example (ringbuf-heavy examples need more)
          # 16MB is often too small once CUDA init + maps are involved.
          SHM_MB=64
          case "$EXAMPLE_NAME" in
            kernelretsnoop) SHM_MB=64 ;;  # ringbuf map needs larger shm
            kernel_trace) SHM_MB=256 ;;
            threadhist) SHM_MB=64 ;;
            threadscheduling) SHM_MB=64 ;;
            atomizer) SHM_MB=64 ;;
            directly_run_on_gpu) SHM_MB=64 ;;
          esac
          # Guard against small /dev/shm (common in containerized runners).
          # If /dev/shm is smaller than requested, shared_memory_object::truncate()
          # will fail with ENOSPC ("No space left on device").
          AVAIL_MB=$(df --output=avail -m /dev/shm 2>/dev/null | tail -1 | tr -d ' ')
          if [[ "${AVAIL_MB:-}" =~ ^[0-9]+$ ]] && [ "$AVAIL_MB" -gt 0 ]; then
            HEADROOM_MB=8
            MAX_MB=$((AVAIL_MB - HEADROOM_MB))
            if [ "$MAX_MB" -lt 32 ]; then
              echo "Not enough /dev/shm available: avail=${AVAIL_MB}MB (need at least 32MB)."
              df -h /dev/shm || true
              ls -lh /dev/shm | head -n 200 || true
              exit 1
            fi
            if [ "$SHM_MB" -gt "$MAX_MB" ]; then
              echo "Clamping BPFTIME_SHM_MEMORY_MB from $SHM_MB to $MAX_MB due to /dev/shm avail=${AVAIL_MB}MB"
              SHM_MB="$MAX_MB"
            fi
          fi
          if [ "$EXAMPLE_MODE" = "direct-run" ]; then
            pushd "$EXAMPLE_DIR" >/dev/null
            echo "Launching direct-run server with $SERVER_SO: $EXE_REL (BPFTIME_SHM_MEMORY_MB=$SHM_MB)"
            # Don't preload into the shell itself; only preload into the target binary.
            BPFTIME_SHM_MEMORY_MB=$SHM_MB BPFTIME_LOG_OUTPUT=console bash -lc "LD_PRELOAD=\"$SERVER_SO\" exec $EXE_REL" > "$ROOT_DIR/server.log" 2>&1 &
            SERVER_PID=$!
            popd >/dev/null
            echo "Server PID: $SERVER_PID"
            sleep 3
            if ! kill -0 "$SERVER_PID" 2>/dev/null; then
              echo "Server crashed early. Showing server.log tail:"; tail -n 200 server.log || true
              exit 1
            fi
            if [ ! -x "$BPFTIMETOOL" ]; then
              echo "bpftimetool not found at $BPFTIMETOOL"; tail -n 200 server.log || true; exit 1
            fi
            echo "Running bpftimetool run-on-cuda (GEMM) ..."
            set +e
            timeout -s SIGKILL ${CLIENT_TIMEOUT}s "$BPFTIMETOOL" run-on-cuda cuda__gemm 1 4 4 1 16 16 1 > tool.log 2>&1
            TOOL_RC=$?
            set -e
            sleep 5
          else
            pushd "$EXAMPLE_DIR" >/dev/null
            echo "Launching server with $SERVER_SO: $EXE_REL (BPFTIME_SHM_MEMORY_MB=$SHM_MB)"
            # Don't preload into the shell itself; only preload into the target binary.
            BPFTIME_SHM_MEMORY_MB=$SHM_MB BPFTIME_LOG_OUTPUT=console bash -lc "LD_PRELOAD=\"$SERVER_SO\" exec $EXE_REL" > "$ROOT_DIR/server.log" 2>&1 &
            SERVER_PID=$!
            popd >/dev/null
            echo "Server PID: $SERVER_PID"
            sleep 5
            if ! kill -0 "$SERVER_PID" 2>/dev/null; then
              echo "Server crashed early. Showing server.log tail:"; tail -n 200 server.log || true
              exit 1
            fi
            echo "Launching client with $AGENT_SO: $VICTIM_REL"
            pushd "$EXAMPLE_DIR" >/dev/null
            set +e
            # Don't preload into the shell itself; only preload into the target binary.
            timeout -s SIGKILL ${CLIENT_TIMEOUT}s env BPFTIME_LOG_OUTPUT=console bash -lc "LD_PRELOAD=\"$AGENT_SO\" exec $VICTIM_REL" > "$ROOT_DIR/client.log" 2>&1
            CLIENT_RC=$?
            set -e
            popd >/dev/null
          fi
          echo "Checking expected output: $EXPECTED"
          # treat crashes as failure even if expected string appears
          if grep -Eiq "(core dumped|Segmentation fault|Aborted)" server.log client.log tool.log; then
            echo "Detected crash in logs"; tail -n 200 server.log || true; tail -n 200 client.log || true; tail -n 200 tool.log || true; RESULT=1
          else
            EXAMPLE_NAME="${{ matrix.examples.name }}"
            FOUND=1
            case "$EXAMPLE_NAME" in
              directly_run_on_gpu)
                # For GEMM, expect a non-zero fixed-point value after bpftimetool ran.
                if grep -Eq "GEMM C\\[0\\]=[1-9]" server.log; then
                  FOUND=0
                fi
                ;;
              cutlass)
                # Counter prints periodically; require a non-zero count to ensure attachment worked.
                if grep -Eq "CUTLASS launches: [1-9]" server.log; then
                  FOUND=0
                fi
                ;;
              cuda-counter)
                if grep -Fq "calls:" server.log || grep -Fq "C[0] =" client.log || grep -Fq "C[1] =" client.log; then
                  FOUND=0
                fi
                ;;
              mem_trace)
                if grep -Fq "counter[0]=" server.log || grep -Fq "mem_traces:" server.log; then
                  FOUND=0
                fi
                ;;
              *)
                if grep -Fq "$EXPECTED" server.log client.log; then
                  FOUND=0
                fi
                ;;
            esac
            if [ $FOUND -eq 0 ]; then
              echo "SUCCESS: found README-like output for $EXAMPLE_NAME"
              RESULT=0
            else
              echo "FAILURE: expected output not found for $EXAMPLE_NAME"
              echo "--- server.log (tail) ---"; tail -n 200 server.log || true
              echo "--- client.log (tail) ---"; tail -n 200 client.log || true
              echo "--- tool.log (tail) ---"; tail -n 200 tool.log || true
              RESULT=1
            fi
          fi
          kill -9 $SERVER_PID 2>/dev/null || true
          wait $SERVER_PID 2>/dev/null || true
          exit $RESULT

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: gpu-example-${{ matrix.examples.name }}-logs
          path: |
            server.log
            client.log
            tool.log
